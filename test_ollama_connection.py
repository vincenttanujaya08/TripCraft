from backend.utils.llm_client import OllamaClient
import sys

def test_ollama():
    print("üîÑ Testing Ollama Connection...")
    
    try:
        client = OllamaClient()
        print(f"‚úÖ Client initialized (Model: {client.model})")
        
        print("‚è≥ Sending test prompt...")
        response = client.generate_content("Say 'Hello TripCraft' and nothing else.")
        
        print(f"üì© Response received: {response.text}")
        
        if "TripCraft" in response.text:
            print("‚úÖ Ollama is working correctly!")
            return True
        else:
            print("‚ö†Ô∏è Response content unexpected but valid.")
            return True
            
    except Exception as e:
        print(f"‚ùå CONNECTION FAILED: {e}")
        print("\nPossible fixes:")
        print("1. Ensure Ollama is running ('ollama serve')")
        print("2. Ensure 'llama3' model is pulled ('ollama pull llama3')")
        return False

if __name__ == "__main__":
    success = test_ollama()
    sys.exit(0 if success else 1)
